<!-- question-type: prepare -->
### Exercise 1: Identifying Issues with Un-tidy Data

We'll start with the deliberately messy version of the ASX stock price data. 

```{r}
asx_prices_messy <- read_csv("data/asx_prices_messy.csv")

asx_prices_messy |> 
    select(gvkey, conm, price_2023, price_2023, 
           eps_2023, eps_2024) |>
    head(10)
```

**(a).** In what ways is this data frame not tidy? Be specific: which tidyâ€‘data principles are violated?

**(b).** Suppose we want the average share price across 2023â€“2024:

$$
\text{avg_price_23_24} = 
\frac{\text{sum_prices_2023} + \text{sum_prices_2024}}
     {\text{count_firms_2023} + \text{count_firms_2024}}
$$
With this messy layout you will need to compute across columns. Fill in the blanks to compute the average share price.

```{r}
#| eval: false

asx_prices_messy |>
    summarise(
        # Sum the prices from the 2023 year
        sum_prices_2023   = sum(YOUR_CODE_HERE, na.rm = TRUE),
        # Sum the prices from the 2024 year
        sum_prices_2024   = sum(YOUR_CODE_HERE, na.rm = TRUE),
        # Count the number of firms with non missing
        # prices in 2023
        count_firms_2023 = sum(!is.na(price_2023)),
        # Count the number of firms with non missing
        # prices in 2024
        count_firms_2024 = sum(!is.na(price_2024))
    ) |>
    mutate(
        # Compute the total sum of prices across both years
        total_sum   = YOUR_CODE_HERE + YOUR_CODE_HERE,
        # Compute the total count of firms across both years
        total_count = count_firms_2023 + count_firms_2024
    ) |>
    mutate(
        # Manually compute the average
        avg_price_23_24 = YOUR_CODE_HERE / YOUR_CODE_HERE
    ) |> 
    select(avg_price_23_24)
```



**(c)** Explain why the approach we took in (b) won't scale as more years of data are added.

**(d)** Examining relationships between variables in a messy data frame can be especially tricky. Suppose we want to see whether share price and earnings per share are correlated in 2024. With this messy structure we must hard-code the year's columns:

```{r}
asx_prices_messy |>
    filter(!is.na(eps_2024)) |>
    ggplot(aes(x = eps_2024, y = price_2024)) +
    geom_point(alpha = 0.4) +
    geom_smooth(method = "lm", se = TRUE, color = "blue") +
    labs(
        title = "Share Price vs EPS (2024)",
        x = "Earnings per Share (EPS, 2024)",
        y = "Share Price (2024)"
    ) +
    ylim(-5, 300) + 
    theme_minimal()
```

What relationship do you see in 2024?
Why does this make sense economically (what are investors "buying" when they invest)?


**(e)**. Now imagine you also wanted to include 2023 in the same analysis. You do not implement code to add these data points â€” just explain the steps you'd need to take.

* Which additional columns would you need to reference in the data?
* How would your plotting code change?
* What problems do you foresee if you wanted to extend this further to 2019â€“2024?



<!-- BEGIN PROFILE:r-teaching-guide -->
::: {.content-visible when-profile="r-teaching-guide"}

::: {.teaching-block}

::: {.teaching-block-header}
Teaching Note
:::

::: {.teaching-block-body}

ðŸŽ¯ **Learning Objective** 
Students should:

- XXXXX

âœ…   **Core Concepts to Highlight**

List them here


ðŸ’¬ **Suggested In-Class Prompts** (if needed)

NA

ðŸ“Œ **Common Misunderstandings**

List as needed

:::

:::

:::
<!-- END PROFILE:r-teaching-guide -->

<!-- BEGIN PROFILE:r-solutions -->
::: {.content-visible when-profile="r-solutions" when-profile="r-teaching-guide"}

::: {.solution-block}

::: {.solution-block-header}
Solution
:::

::: {.solution-block-body}

**(a). Why this data frame is â€œmessyâ€**

- Data is stored in column names: columns like `price_2020` or `eps_2023` contain values (years) that should instead appear in a variable column (e.g., `fyear`). This violates the principles that â€œcolumn names are variables, not valuesâ€ and "each value must have its own cell."

- Two variables are encoded in column names: each column like `price_2020` or `eps_2023` mixes the measure (price vs EPS) and the year (2020, 2023). This violates â€œeach variable has its own column.â€

- One observation is split across many columns: a single firmâ€“year record is spread over separate columns for each year, rather than one row per firmâ€“year. This violates â€œeach observation (firmâ€“year) has its own row.â€

- Heterogeneous columns for the same concept: all the `price_*` columns represent the same variable (price) measured in different years, but they appear as many columns instead of one `price` column with a `fyear` identifier.

**(b)**.

```{r}
asx_prices_messy |>
    summarise(
        # Sum the prices from the 2023 year
        sum_prices_2023   = sum(price_2023, na.rm = TRUE),
        # Sum the prices from the 2024 year
        sum_prices_2024   = sum(price_2024, na.rm = TRUE),
        # Count the number of firms with non missing
        # prices in 2023
        count_firms_2023 = sum(!is.na(price_2023)),
        # Count the number of firms with non missing
        # prices in 2024
        count_firms_2024 = sum(!is.na(price_2024))
    ) |>
    mutate(
        # Compute the total sum of prices across both years
        total_sum   = sum_prices_2023 + sum_prices_2024,
        # Compute the total count of firms across both years
        total_count = count_firms_2023 + count_firms_2024
    ) |>
    mutate(
        # Manually compute the average
        avg_price_23_24 = total_sum / total_count
    ) |> 
    select(avg_price_23_24)
```

**(c)**.
The approach in (b) doesn't scale because each new year of data adds a new column (e.g., `price_2022`, `price_2023`), which requires manually updating the code in four different places: two sums and two counts. This becomes tedious and error-prone as the number of years grows.


**(d)**. 
In 2024, the plot shows a positive relationship between earnings per share (EPS) and share price. This makes sense because investors are generally willing to pay more for companies that earn more per share â€” higher earnings often signal better profitability, which increases a firmâ€™s value.

Economically, this reflects the idea that when investors buy a share, theyâ€™re buying a claim on future earnings. So firms with higher EPS tend to attract more investor demand, pushing up their share price.

**(e)**. 

To include 2023 in the same analysis, you'd need to manually reference two more columns: eps_2023 and price_2023. You would then need to copy and adapt your plotting code to create a second scatterplot or combine the data in some custom way.

Your code would get much longer and harder to manage, especially if you wanted to compare more years (e.g., 2019â€“2024). For each new year, you'd have to:

* Add new x and y column references (e.g., eps_2022, price_2022)
* Manually stack or merge the data across years
* Repeat or complicate the plotting code

This shows how the column-based structure becomes inefficient and messy, making it hard to scale analyses or visualizations.
:::

:::

:::
<!-- END PROFILE:r-solutions -->
