<!-- question-type: prepare -->
### Exercise 1: Identifying Issues with Un-tidy Data

We'll start with a deliberately messy version of the ASX stock price data. Run the following code to load and implement some preliminary manipulations this data set.

```{r}
#| eval: false
asx_prices_messy <- read_csv("data/asx_prices_messy.csv")
```

```{r}
asx_prices_messy |> 
    select(gvkey, conm, price_2023, price_2024, 
           eps_2023, eps_2024) |>
    head(10)
```

**(a).** In what ways is this data frame not tidy? Be specific: which tidy‚Äëdata principles are violated?

**(b).** Suppose we want the average share price across all firms for the period 2023‚Äì2024. With this messy layout you will need to compute sum and counts across columns (i.e., sum prices for 2023, for 2024; then, count non-missing observations in these columns for 2023, 2024, etc). Use the starter code below to compute the average share price across all firms for the period 2023-2024.

```{r}
#| eval: false

asx_prices_messy |>
    summarise(
        # Sum the prices from the 2023 year
        sum_prices_2023   = sum(YOUR_CODE_HERE, na.rm = TRUE),
        # Sum the prices from the 2024 year
        sum_prices_2024   = sum(YOUR_CODE_HERE, na.rm = TRUE),
        # Count the number of firms with non missing
        # prices in 2023
        count_firms_2023 = sum(!is.na(price_2023)),
        # Count the number of firms with non missing
        # prices in 2024
        count_firms_2024 = sum(!is.na(price_2024))
    ) |>
    mutate(
        # Compute the total sum of prices across both years
        total_sum   = YOUR_CODE_HERE + YOUR_CODE_HERE,
        # Compute the total count of firms across both years
        total_count = count_firms_2023 + count_firms_2024
    ) |>
    mutate(
        # Manually compute the average
        avg_price_23_24 = YOUR_CODE_HERE / YOUR_CODE_HERE
    ) |> 
    select(avg_price_23_24)
```

**(c)** Explain why the approach we took in (b) doesn't scale well as more years of data are added.

**(d)** Examining relationships between variables in a messy data frame can be especially tricky. Suppose we want to see whether share price and earnings per share are correlated in 2024. With this messy structure we must hard-code the year's columns:

```{r}
#| warning: false

asx_prices_messy |>
    filter(eps_2024 > -5) |>
    ggplot(aes(x = eps_2024, y = price_2024)) +
    geom_point(alpha = 0.4) +
    geom_smooth(method = "lm", se = TRUE, color = "blue") +
    labs(
        title = "Share Price vs EPS (2024)",
        x = "Earnings per Share (EPS, 2024)",
        y = "Share Price (2024)"
    ) +
    theme_minimal()
```

Based on this plot, what relationship do you see in 2024?
Why does this make sense economically (what are investors "buying" when they invest)?

**(e)**. Now imagine you also wanted to include 2023 in the same analysis. You do not need to implement code to add these data points to the plot above ‚Äî just explain the steps you'd need to take.

* Which additional columns would you need to reference in the data?
* How would your plotting code change?
* What problems do you foresee if you wanted to extend this further to 2019‚Äì2024?



<!-- BEGIN PROFILE:r-teaching-guide -->
::: {.content-visible when-profile="r-teaching-guide"}

::: {.teaching-block}

::: {.teaching-block-header}
Teaching Note
:::

::: {.teaching-block-body}

üéØ **Learning Objective** 
Students should:

- Identify and explain why the provided data is not tidy.

- Practise reshaping messy data structures to make analysis scalable.

- Gain hands-on experience computing summaries and relationships when variables are spread across columns.

‚úÖ   **Core Concepts to Highlight**

Tidy data rules: one variable per column, one observation per row.

Scalability: how wide data (year-specific columns) leads to repetitive, brittle code.

Wrangling tools: the role of `pivot_longer()` and joins in cleaning and combining datasets.


üí¨ **Suggested In-Class Prompts** (if needed)

NA

üìå **Common Misunderstandings**

Thinking that ‚Äúmessy‚Äù just means ugly formatting, rather than a structural problem.

Assuming that writing more `summarise()` lines is fine, without realizing it becomes unmanageable.

Confusing the role of `select()`/`summarise()` (short-term fixes) with the need for reshaping (`pivot_longer()`) as a long-term solution.

:::

:::

:::
<!-- END PROFILE:r-teaching-guide -->

<!-- BEGIN PROFILE:r-solutions -->
::: {.content-visible when-profile="r-solutions" when-profile="r-teaching-guide"}

::: {.solution-block}

::: {.solution-block-header}
Solution
:::

::: {.solution-block-body}

**(a). Why this data frame is ‚Äúmessy‚Äù**

- Data is stored in column names: columns like `price_2020` or `eps_2023` contain values (years) that should instead appear in a variable column (e.g., `fyear`). This violates the principles that ‚Äúcolumn names are variables, not values‚Äù and "each value must have its own cell."

- Two variables are encoded in column names: each column like `price_2020` or `eps_2023` mixes the measure (price vs EPS) and the year (2020, 2023). This violates ‚Äúeach variable has its own column.‚Äù

- One observation is split across many columns: a single firm‚Äìyear record is spread over separate columns for each year, rather than one row per firm‚Äìyear. This violates ‚Äúeach observation (firm‚Äìyear) has its own row.‚Äù

- Heterogeneous columns for the same concept: all the `price_*` columns represent the same variable (price) measured in different years, but they appear as many columns instead of one `price` column with a `fyear` identifier.

**(b)**.

```{r}
asx_prices_messy |>
    summarise(
        # Sum the prices from the 2023 year
        sum_prices_2023   = sum(price_2023, na.rm = TRUE),
        # Sum the prices from the 2024 year
        sum_prices_2024   = sum(price_2024, na.rm = TRUE),
        # Count the number of firms with non missing
        # prices in 2023
        count_firms_2023 = sum(!is.na(price_2023)),
        # Count the number of firms with non missing
        # prices in 2024
        count_firms_2024 = sum(!is.na(price_2024))
    ) |>
    mutate(
        # Compute the total sum of prices across both years
        total_sum   = sum_prices_2023 + sum_prices_2024,
        # Compute the total count of firms across both years
        total_count = count_firms_2023 + count_firms_2024
    ) |>
    mutate(
        # Manually compute the average
        avg_price_23_24 = total_sum / total_count
    ) |> 
    select(avg_price_23_24)
```

**(c)**.
The approach in (b) doesn't scale because each new year of data adds a new column (e.g., `price_2022`, `price_2023`), which requires manually updating the code in four different places: two sums and two counts. This becomes tedious and error-prone as the number of years grows.


**(d)**. 
In 2024, the plot shows a positive relationship between earnings per share (EPS) and share price. This makes sense because investors are generally willing to pay more for companies that earn more per share ‚Äî higher current earnings often signal better future earnings, which increases a firm‚Äôs value.

Economically, this reflects the idea that when investors buy a share, they‚Äôre buying a claim on future earnings. So firms with higher EPS tend to attract more investor demand, pushing up their share price.

**(e)**. 

To include 2023 in the same analysis, you'd need to manually reference two more columns: `eps_2023` and `price_2023.` You would then need to copy and adapt your plotting code to create a second scatterplot or combine the data in some custom way.

Your code would get much longer and harder to manage, especially if you wanted to compare more years (e.g., 2019‚Äì2024). For each new year, you'd have to:

* Add new x and y column references (e.g., `eps_2022`, `price_2022`)
* Manually stack or merge the data across years
* Repeat or complicate the plotting code

This shows how the column-based structure becomes inefficient and messy, making it hard to scale analyses or visualizations.
:::

:::

:::
<!-- END PROFILE:r-solutions -->
